ğŸš€ Containerized ML Image Classification API with CI/CD
ğŸ“Œ Project Overview

This project demonstrates how to deploy a production-ready Machine Learning inference service using:

âœ… TensorFlow/Keras model

âœ… FastAPI RESTful API

âœ… Docker containerization

âœ… Docker Compose for local development

âœ… GitHub Actions CI/CD pipeline

âœ… Automated unit testing with Pytest

The application exposes a /predict endpoint that accepts image uploads and returns classification predictions in JSON format.

This project bridges the gap between model development and real-world deployment â€” a core skill in MLOps and Machine Learning Engineering.

ğŸ§  Features

RESTful API with FastAPI

Health check endpoint (GET /health)

Image classification endpoint (POST /predict)

Robust input validation and error handling

Model loaded once at application startup (optimized inference)

Structured logging

Multi-stage optimized Docker build

Docker Compose support

GitHub Actions CI/CD automation

Unit tests using pytest

Prediction output artifacts

ğŸ›  Technology Stack

Python 3.9

TensorFlow / Keras

FastAPI

Uvicorn

Docker

Docker Compose

GitHub Actions

Pytest

Pillow

NumPy

ğŸ“ Project Structure
your-ml-api/
â”œâ”€â”€ .github/workflows/main.yml
â”œâ”€â”€ models/
â”œâ”€â”€ predictions/
â”œâ”€â”€ scripts/train_model.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ model.py
â”œâ”€â”€ tests/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md

âš™ï¸ Setup Instructions (Local Development)
1ï¸âƒ£ Clone Repository
git clone https://github.com/Kusubhavani/your-ml-api
cd your-ml-api

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Train & Generate Model
python scripts/train_model.py


This generates:

models/my_classifier_model.h5

4ï¸âƒ£ Run API Locally (Without Docker)
uvicorn src.main:app --reload


Access:

Swagger UI â†’ http://localhost:8000/docs

Health â†’ http://localhost:8000/health

ğŸ³ Run with Docker (Recommended)
Build & Start
docker-compose up --build


API available at:

http://localhost:8000

ğŸ” API Usage Examples
Health Check
curl http://localhost:8000/health


Response:

{
  "status": "ok",
  "message": "API is healthy and model is loaded."
}

Image Prediction
curl -X POST "http://localhost:8000/predict" \
  -F "file=@cat.png"


Response:

{
  "class_label": "cat",
  "probabilities": [0.1, 0.05, 0.8, ...]
}

ğŸ§ª Running Tests
pytest tests/


Tests cover:

Health endpoint

Prediction endpoint

Input validation

Mocked model inference

ğŸ” CI/CD Pipeline (GitHub Actions)

Located in:

.github/workflows/main.yml

Triggered On:

Push to main

Pull request to main

Pipeline Steps:

Checkout repository

Setup Python

Install dependencies

Run tests

Build Docker image

Tag image with commit SHA

Simulated push to registry

Upload prediction artifacts

This ensures:

Code quality

Automated validation

Deployment readiness

ğŸŒ Environment Variables

Defined in .env.example:

MODEL_PATH=models/my_classifier_model.h5
LOG_LEVEL=INFO


Used for:

Configurable model path

Logging level

ğŸ“Š Predictions Directory

The predictions/ folder contains example JSON outputs generated by successful /predict calls.

Example:

{
  "class_label": "cat",
  "probabilities": [0.8, 0.2]
}


These are also uploaded as CI/CD artifacts.

ğŸ³ Docker Optimization Techniques Used

Multi-stage build

Slim base image (python:3.9-slim-buster)

Dependency layer caching

Minimal runtime image

Exposed only required port (8000)

ğŸ“ˆ Performance Considerations

Model loaded once on startup

No per-request model reloading

Efficient image preprocessing

Proper error handling to avoid crashes

Structured logging for observability

ğŸš€ Future Enhancements

JWT Authentication

Model versioning

Kubernetes deployment

GPU inference support

Prometheus monitoring

Centralized logging

Auto-scaling support

Model performance tracking

ğŸ¯ Why This Project Matters

This project demonstrates:

End-to-end ML deployment

MLOps fundamentals

Production-grade API design

Docker best practices

CI/CD automation

Clean architecture

Error handling & validation

Model serving optimization

These are essential skills for:

Machine Learning Engineers

MLOps Engineers

Backend AI Developers

ğŸ“œ License

This project is created for educational and portfolio purposes.

ğŸ‘¨â€ğŸ’» Author

KUSU BHAVANI